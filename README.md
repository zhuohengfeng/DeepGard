<p align="center">
  <img src="logo.png" width="200" alt="DeepGrad Logo"/>
</p>

<p align="center"><em>A lightweight and modular deep learning framework</em></p>

---

## ğŸš€ Features

- ğŸ” **Automatic Differentiation** (autograd engine built from scratch)
- ğŸ§± **Modular Layers** (build models layer by layer)
- ğŸ¯ **Optimizers** (SGD, Adam, RMSProp etc.)
- ğŸ“Š **Metrics & Losses** (MSE, CrossEntropy, etc.)
- ğŸ§ª **Custom Training Loops** with flexibility
- ğŸ§µ **Numpy-based** backend (no heavy dependencies)

---

## ğŸ§  Quick Start

```python
from deepgrad import Tensor, Linear, MSELoss, SGD

# Dummy training example
x = Tensor([[1.0], [2.0]])
y = Tensor([[2.0], [4.0]])

model = Linear(1, 1)
loss_fn = MSELoss()
optimizer = SGD(model.parameters(), lr=0.01)

for epoch in range(100):
    pred = model(x)
    loss = loss_fn(pred, y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print(f"Epoch {epoch}, Loss: {loss.data:.4f}")
